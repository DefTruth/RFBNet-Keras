{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "import copy\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "#From keras\n",
    "import keras\n",
    "from keras.models import load_model\n",
    "# from keras.applications.mobilenet_v2 import preprocess_input, decode_predictions\n",
    "from keras import optimizers\n",
    "from keras.utils import multi_gpu_model\n",
    "from keras import backend as K\n",
    "from keras.callbacks import *\n",
    "# From ssd_keras\n",
    "from data_generator.object_detection_2d_data_generator import DataGenerator\n",
    "from data_generator.data_augmentation_chain_original_ssd import SSDDataAugmentation\n",
    "from data_generator.object_detection_2d_geometric_ops import Resize\n",
    "from data_generator.object_detection_2d_photometric_ops import ConvertTo3Channels\n",
    "from data_generator.data_augmentation_chain_original_ssd import SSDDataAugmentation\n",
    "from data_generator.object_detection_2d_misc_utils import apply_inverse_transforms\n",
    "from ssd_encoder_decoder.ssd_input_encoder import SSDInputEncoder\n",
    "import ssd_encoder_decoder.ssd_output_decoder\n",
    "from ssd_encoder_decoder.ssd_output_decoder import decode_detections\n",
    "from SSD_loss import SSDLoss\n",
    "from eval_utils.average_precision_evaluator import Evaluator\n",
    "from importlib import reload\n",
    "#import cv2\n",
    "from data_augment import LabelEncoder\n",
    "from detector_help import process_y,post_process,prior_box\n",
    "import detection_nets,classification_nets\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "from drawing import *\n",
    "from detection_nets import load_mobilenetv2\n",
    "%matplotlib inline\n",
    "import debug_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set model config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_H = 300\n",
    "input_W = 300\n",
    "input_C = 3\n",
    "input_shape = (input_H, input_W, input_C)\n",
    "model_name = \"rfblite_mn2_300_inverted redisual\"\n",
    "root_path = \"/home/cai/dataset/VOCdevkit\"\n",
    "aspect_ratios = [[2,3],\n",
    "                 [2,3],\n",
    "                 [2,3],\n",
    "                 [2,3],\n",
    "                 [2],\n",
    "                 [2]]\n",
    "prior_config =  [2 + len(ar) * 2 for ar in aspect_ratios]  # number of boxes per feature map location\n",
    "source_layers = [\"block_12_expand\"] #[\"block_6_expand\",\"block_12_expand\"]\n",
    "#Extra layer head:  \n",
    "#rfblite_d  rfblite_d, oup, dilation_base, dilation_rate, only_bone,source\n",
    "#plain      oup, stride,pad,source\n",
    "extra_config = [\n",
    "               ['literfb_d',512, 1,  2, False, True],\n",
    "               [256, 2, 'same', True],\n",
    "               [256,  1, 'same', True],\n",
    "               [128,  1, 'valid', True]]\n",
    "source_config = [['literfb_d',96, 1, 2]]\n",
    "mean_color = [123, 117, 104] \n",
    "swap_channels = [2, 1, 0] # The color channel order in the original SSD is BGR, so we'll have the model reverse the color channel order of the input images.\n",
    "num_classes = 20 # Number of positive classes, e.g. 20 for Pascal VOC, 80 for MS COCO\n",
    "variances = [0.1, 0.1, 0.2, 0.2]\n",
    "# scale = [0.1, 0.2, 0.37, 0.54, 0.71, 0.88, 1.05]\n",
    "scale = [0.07, 0.17, 0.33, 0.51, 0.67, 0.87, 1.05]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cai/.local/lib/python3.5/site-packages/keras/engine/saving.py:310: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[256, 2, 'same', True]\n",
      "[256, 1, 'same', True]\n",
      "[128, 1, 'valid', True]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'source_cfg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-53fc51bae189>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m               \u001b[0mnum_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m               \u001b[0mlite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m               return_predictor = True)\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m                       \u001b[0;31m#Comment this line if you don`t want to show summary every time you build your model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/project/RFBNet-Keras/detection_nets.py\u001b[0m in \u001b[0;36mbuild_RFBLite\u001b[0;34m(input_shape, phase, source_layers, base_model, extra_config, source_config, include_base, prior_config, num_classes, source_expand_ratio, source_dep_mul, base_index, lite, show_summary, l2_reg, return_predictor)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0;31m# 4. Add RFBLite for layer from sources\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcfg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_cfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'literfb_d'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m                 \u001b[0mfilters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0monly_bone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdil_base\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdil_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcfg\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'source_cfg' is not defined"
     ]
    }
   ],
   "source": [
    "reload(detection_nets)\n",
    "##Load basenet first\n",
    "K.clear_session()\n",
    "base_model = load_mobilenetv2(size = 300)\n",
    "build_model = detection_nets.build_RFBLite\n",
    "adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "ssd_loss = SSDLoss(neg_pos_ratio=3,alpha=1.0)\n",
    "\n",
    "model = build_model(input_shape = (300,300,3),\n",
    "              phase = 'train',\n",
    "              source_layers = source_layers,\n",
    "              base_model = base_model,\n",
    "              extra_config = extra_config,\n",
    "              source_config = source_config,\n",
    "              include_base = True,\n",
    "              base_index = -1,\n",
    "              source_expand_ratio = 1,\n",
    "              source_dep_mul = 1,\n",
    "              prior_config = prior_config,\n",
    "              num_classes = num_classes,\n",
    "              lite = True,\n",
    "              return_predictor = True)\n",
    "\n",
    "model.summary()                       #Comment this line if you don`t want to show summary every time you build your model\n",
    "# model.load_weights(\"saved_weights/BEST_rfblite_mn2_300_b_pascal_07+12_epoch-122_loss-2.1461_val_loss-2.6892.h5\",by_name = True)\n",
    "# print(get_flops())\n",
    "model = detection_nets.preprocess(input_shape,model,mean_color,swap_channels)\n",
    "# model = multi_gpu_model(model,gpus=gpus) #Comment it if don`t use multi_gpu \n",
    "\n",
    "#Don`t tune learning rate here because we will use a lr scheduler in callbacks\n",
    "#Orignal paper used SGD but according to the author of ssd_keras, use adam is better \n",
    "model.compile(adam,\n",
    "              loss=ssd_loss.loss,\n",
    "              metrics = [ssd_loss.class_loss,ssd_loss.loc_loss]\n",
    "              )\n",
    "print(model_name,'has been built ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show a analysis of params "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(debug_model)\n",
    "keys = [\"rfb_source_0\",\"rfb_source_1\",\"rfb_extra_0\",\"plain\",\"loc\",\"conf\"] \n",
    "debug_model.model_params_bar(model,16*9+4,len(model.layers),0,keys = keys)\n",
    "debug_model.model_params_of_layer(model,16*9+4,len(model.layers),0,key = 'rfb_source_0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l=model.layers[-1].get_layer(\"rfb_source_0_conv3_2_sep\")\n",
    "x = Input((19,19,1152))\n",
    "y = l(x)\n",
    "print(l.filters,l.kernel_size,l.padding,l.depth_multiplier)\n",
    "m = Model(x,y)\n",
    "m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "x = Input((19,19,1152))\n",
    "y = SeparableConv2D(1152,3)(x)\n",
    "m = Model(x,y)\n",
    "m.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load previously trained models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session() # Clear previous models from memory.\n",
    "\n",
    "model_path = \"saved_weights/rfblite_mn2_300_b_pascal_07+12_epoch-122_loss-2.1461_val_loss-2.6892.h5\"\n",
    "adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "ssd_loss = SSDLoss(neg_pos_ratio=3,alpha=1.0)\n",
    "\n",
    "# model.load_weights(model_path)\n",
    "# model = multi_gpu_model(model,gpus=2) #Comment it if don`t use multi_gpu \n",
    "model = load_model(model_path,custom_objects={'loss': ssd_loss.loss,'class_loss':ssd_loss.class_loss,'loc_loss':ssd_loss.loc_loss})\n",
    "print(\"load model from\",model_path)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_map = [layer.output.shape.as_list()[1:3] for layer in model.layers[-1].layers if ('loc_' in layer.name and 'conv' in layer.name)]\n",
    "priors = prior_box(feature_map,aspect_ratios,scale = scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speed Test(optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "batch_n = 100\n",
    "img_height = 300\n",
    "img_width = 300\n",
    "img_channels = 3\n",
    "# detector = Detect(num_classes,0,cfg)\n",
    "batch_size_range = [1,2,4,8]#[1,2,4,8]\n",
    "print('Start testing...')\n",
    "for batch_size in batch_size_range:\n",
    "    time_total = 0\n",
    "    time_net = 0\n",
    "    time_post = 0\n",
    "    print('batch size',batch_size)\n",
    "    for i in range(0,batch_n):\n",
    "        test_batch = np.random.rand(batch_size,img_height,img_width,img_channels)\n",
    "        click = time.time()\n",
    "        y_pred = model.predict(test_batch)\n",
    "        time_net += (time.time() - click)\n",
    "        click = time.time()\n",
    "        y_pred_decoded = decode_detections(y_pred,\n",
    "                          priors,variances,\n",
    "                          img_height = input_H,\n",
    "                          img_width = input_W,\n",
    "                          confidence_thresh = 0.5,\n",
    "                          iou_threshold = 0.45)\n",
    "        time_post += (time.time() - click)\n",
    "    time_total = time_net + time_post\n",
    "    print('Time cost per batch: %.3f FPS: %.1f'%(time_total / batch_n,batch_n*batch_size/ time_total))\n",
    "    print('Time(pure forward) cost per batch: %.3f FPS: %.1F'%(time_net / batch_n, batch_n*batch_size/ time_net))\n",
    "batch_size = None\n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Define class_names\n",
    "class_names = ['background',\n",
    "           'aeroplane', 'bicycle', 'bird', 'boat',\n",
    "           'bottle', 'bus', 'car', 'cat',\n",
    "           'chair', 'cow', 'diningtable', 'dog',\n",
    "           'horse', 'motorbike', 'person', 'pottedplant',\n",
    "           'sheep', 'sofa', 'train', 'tvmonitor']\n",
    "\n",
    "force_create_dataset = False #Force data generator to load dataset from source\n",
    "root_path = '/home/cai/dataset/VOCdevkit'\n",
    "trainset_hdf5_path = 'dataset_pascal_voc_07+12_trainval.h5'\n",
    "valset_hdf5_path = 'dataset_pascal_voc_07_test.h5'\n",
    "\n",
    "# The directories that contain the images.\n",
    "VOC_2007_images_dir      = root_path + '/VOC2007/JPEGImages/'\n",
    "VOC_2012_images_dir      = root_path + '/VOC2012/JPEGImages/'\n",
    "\n",
    "# The directories that contain the annotations.\n",
    "VOC_2007_annotations_dir      = root_path + '/VOC2007/Annotations/'\n",
    "VOC_2012_annotations_dir      = root_path + '/VOC2012/Annotations/'\n",
    "\n",
    "# The paths to the image sets.\n",
    "VOC_2007_train_image_set_filename    = root_path + '/VOC2007/ImageSets/Main/train.txt'\n",
    "VOC_2012_train_image_set_filename    = root_path + '/VOC2012/ImageSets/Main/train.txt'\n",
    "VOC_2007_val_image_set_filename      = root_path + '/VOC2007/ImageSets/Main/val.txt'\n",
    "VOC_2012_val_image_set_filename      = root_path + '/VOC2012/ImageSets/Main/val.txt'\n",
    "VOC_2007_trainval_image_set_filename = root_path + '/VOC2007/ImageSets/Main/trainval.txt'\n",
    "VOC_2012_trainval_image_set_filename = root_path + '/VOC2012/ImageSets/Main/trainval.txt'\n",
    "VOC_2007_test_image_set_filename     = root_path + '/VOC2007/ImageSets/Main/test.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(trainset_hdf5_path) and os.path.isfile(valset_hdf5_path) and not force_create_dataset:\n",
    "    #Load dataset from created hdf5_dataset\n",
    "    train_dataset = DataGenerator(hdf5_dataset_path = trainset_hdf5_path)\n",
    "    val_dataset = DataGenerator(hdf5_dataset_path = valset_hdf5_path)\n",
    "else:\n",
    "    train_dataset = DataGenerator()\n",
    "    val_dataset = DataGenerator()\n",
    "    \n",
    "    train_dataset.parse_xml(images_dirs=[VOC_2007_images_dir,\n",
    "                                         VOC_2012_images_dir],\n",
    "                            image_set_filenames=[VOC_2007_trainval_image_set_filename,\n",
    "                                                 VOC_2012_trainval_image_set_filename],\n",
    "                            annotations_dirs=[VOC_2007_annotations_dir,\n",
    "                                              VOC_2012_annotations_dir],\n",
    "                            classes=class_names,\n",
    "                            include_classes='all',\n",
    "                            exclude_truncated=False,\n",
    "                            exclude_difficult=False,\n",
    "                            ret=False)\n",
    "\n",
    "    val_dataset.parse_xml(images_dirs=[VOC_2007_images_dir],\n",
    "                          image_set_filenames=[VOC_2007_test_image_set_filename],\n",
    "                          annotations_dirs=[VOC_2007_annotations_dir],\n",
    "                          classes=class_names,\n",
    "                          include_classes='all',\n",
    "                          exclude_truncated=False,\n",
    "                          exclude_difficult=True,\n",
    "                          ret=False)\n",
    "    train_dataset.create_hdf5_dataset(file_path=trainset_hdf5_path,\n",
    "                                      resize=False,\n",
    "                                      variable_image_size=True,\n",
    "                                      verbose=True)\n",
    "\n",
    "    val_dataset.create_hdf5_dataset(file_path=valset_hdf5_path,\n",
    "                                    resize=False,\n",
    "                                    variable_image_size=True,\n",
    "                                    verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import detector_help\n",
    "reload(detector_help)\n",
    "from detector_help import *\n",
    "import data_augment\n",
    "reload(data_augment)\n",
    "from data_augment import *\n",
    "\n",
    "batch_size = 32\n",
    "ssd_data_augmentation = SSDDataAugmentation(img_height=input_H,\n",
    "                                            img_width=input_W,\n",
    "                                            background=mean_color)\n",
    "\n",
    "# For the validation generator:\n",
    "convert_to_3_channels = ConvertTo3Channels()\n",
    "resize = Resize(height=input_H, width=input_W)\n",
    "\n",
    "# 5: Instantiate an encoder that can encode ground truth labels into the format needed by the SSD loss function.\n",
    "\n",
    "# The encoder constructor needs the spatial dimensions of the model's predictor layers to create the anchor boxes.\n",
    "label_encoder = LabelEncoder(num_classes,priors,variances,input_H,input_W)\n",
    "\n",
    "# 6: Create the generator handles that will be passed to Keras' `fit_generator()` function.\n",
    "\n",
    "train_generator = train_dataset.generate(batch_size=batch_size,\n",
    "                                         shuffle=True,\n",
    "                                         transformations=[ssd_data_augmentation],\n",
    "                                         label_encoder=label_encoder,\n",
    "                                         returns={'processed_images',\n",
    "                                                  'encoded_labels'},\n",
    "                                         keep_images_without_gt=False)\n",
    "\n",
    "val_generator = val_dataset.generate(batch_size=batch_size,\n",
    "                                     shuffle=False,\n",
    "                                     transformations=[convert_to_3_channels,\n",
    "                                                      resize],\n",
    "                                     label_encoder=label_encoder,\n",
    "                                     returns={'processed_images',\n",
    "                                              'encoded_labels'},\n",
    "                                     keep_images_without_gt=False)\n",
    "\n",
    "# Get the number of samples in the training and validations datasets.\n",
    "train_dataset_size = train_dataset.get_dataset_size()\n",
    "val_dataset_size   = val_dataset.get_dataset_size()\n",
    "print(\"Number of priors:\\t{:>6}\".format(len(priors)))\n",
    "print(\"Number of images in the training dataset:\\t{:>6}\".format(train_dataset_size))\n",
    "print(\"Number of images in the validation dataset:\\t{:>6}\".format(val_dataset_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Draw a batch of images(optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import drawing\n",
    "reload(drawing)\n",
    "from drawing import *\n",
    "train_flow_origin = train_dataset.generate(batch_size = 4,\n",
    "                                           transformations=[], \n",
    "                                           label_encoder = label_encoder,\n",
    "                                           returns = ['original_images','original_labels'])\n",
    "images,labels = next(train_flow_origin)\n",
    "for img,label in zip(images, labels):\n",
    "    draw_detection(img,label,class_names,pred_format = {\"class\":0,\"xmin\":1,\"ymin\":2,\"xmax\":3,\"ymax\":4})           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set training params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If load weights from files,initial_epoch need to be set to the next epoch to be trained.\n",
    "initial_epoch = 0\n",
    "final_epochs = 120\n",
    "steps_per_epoch = 1000\n",
    "plot = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define callback function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Set file path\n",
    "weights_folder = 'saved_weights/'\n",
    "\n",
    "if not os.path.isdir(weights_folder):\n",
    "    os.mkdir(weights_folder)\n",
    "\n",
    "checkpoint_filepath = weights_folder + model_name + '_pascal_07+12_epoch-{epoch:02d}_loss-{loss:.4f}_val_loss-{val_loss:.4f}.h5'\n",
    "log_filepath = \"training_summaries/\" +model_name + '_pascal_07+12_training_log.csv' \n",
    "#2. Define a lr_scheduler\n",
    "\n",
    "reduce_learning_rate = ReduceLROnPlateau(monitor='val_loss',\n",
    "                                         factor=0.2,\n",
    "                                         patience=8,\n",
    "                                         verbose=1,\n",
    "                                         epsilon=0.001,\n",
    "                                         cooldown=0,\n",
    "                                         min_lr=0.00001)    \n",
    "#3. ## Define callbacks\n",
    "model_checkpoint = ModelCheckpoint(filepath = checkpoint_filepath,\n",
    "                                   monitor='val_loss',\n",
    "                                   verbose=1,\n",
    "                                   save_best_only=True,\n",
    "                                   save_weights_only=True,\n",
    "                                   mode='auto',\n",
    "                                   period=5)\n",
    "\n",
    "csv_logger = CSVLogger(filename=log_filepath,\n",
    "                           separator=',',\n",
    "                           append=True)\n",
    "\n",
    "terminate_on_nan = TerminateOnNaN()\n",
    "\n",
    "callbacks = [model_checkpoint,\n",
    "             csv_logger,\n",
    "             reduce_learning_rate,\n",
    "             terminate_on_nan]\n",
    "# callbacks = [csv_logger,learning_rate_scheduler]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('model name' ,model_name , \\\n",
    "       '\\nstart fitting.. ' , \n",
    "       'inital epoch:', initial_epoch,\n",
    "       'final epoch:', final_epochs,\n",
    "       'epoch step:', steps_per_epoch,\n",
    "       'plot_history:' ,plot, \\\n",
    "       '\\ntrainset size:',  train_dataset.get_dataset_size(), ' batch_size', batch_size )     \n",
    "\n",
    "validation_steps =  val_dataset.get_dataset_size() // batch_size\n",
    "history = model.fit_generator(train_generator,\n",
    "                              use_multiprocessing = False, \n",
    "                              steps_per_epoch = steps_per_epoch, \n",
    "                              validation_data = val_generator,\n",
    "                              validation_steps = validation_steps, \n",
    "                              epochs = final_epochs,\n",
    "                              callbacks = callbacks,\n",
    "                              initial_epoch = initial_epoch)\n",
    "\n",
    "if plot:\n",
    "    plot_history(history)\n",
    "    plt.savefig('./%s_pascal07+12.jpg'%model_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mAP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import eval_utils.average_precision_evaluator\n",
    "reload(eval_utils.average_precision_evaluator)\n",
    "from eval_utils.average_precision_evaluator import Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = DataGenerator()\n",
    "test_dataset.parse_xml(images_dirs=[VOC_2007_images_dir],\n",
    "                      image_set_filenames=[VOC_2007_test_image_set_filename],\n",
    "                      annotations_dirs=[VOC_2007_annotations_dir],\n",
    "                      classes=class_names,\n",
    "                      include_classes='all',\n",
    "                      exclude_truncated=False,\n",
    "                      exclude_difficult=False,\n",
    "                      ret=False)\n",
    "\n",
    "print('Test size', test_dataset.get_dataset_size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Evaluator(model=model,\n",
    "                      n_classes=num_classes,\n",
    "                      data_generator=test_dataset,\n",
    "                      model_mode='training')\n",
    "\n",
    "results = evaluator(img_height=input_H,\n",
    "                    img_width=input_W,\n",
    "                    batch_size=batch_size,\n",
    "                    priors = priors,\n",
    "                    variances = variances,\n",
    "                    data_generator_mode='resize',\n",
    "                    round_confidences=False,\n",
    "                    matching_iou_threshold=0.5,\n",
    "                    border_pixels='include',\n",
    "                    sorting_algorithm='quicksort',\n",
    "                    average_precision_mode='sample',\n",
    "                    num_recall_points=11,\n",
    "                    ignore_neutral_boxes=True,\n",
    "                    return_precisions=True,\n",
    "                    return_recalls=True,\n",
    "                    return_average_precisions=True,\n",
    "                    verbose=True)\n",
    "\n",
    "mean_average_precision, average_precisions, precisions, recalls = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(1, len(average_precisions)):\n",
    "    print(\"{:<14}{:<6}{}\".format(class_names[i], 'AP', round(average_precisions[i], 3)))\n",
    "print()\n",
    "print(\"{:<14}{:<6}{}\".format('','mAP', round(mean_average_precision, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "m = max((num_classes + 1) // 2, 2)\n",
    "n = 2\n",
    "\n",
    "fig, cells = plt.subplots(m, n, figsize=(n*8,m*8))\n",
    "for i in range(m):\n",
    "    for j in range(n):\n",
    "        if n*i+j+1 > num_classes: break\n",
    "        cells[i, j].plot(recalls[n*i+j+1], precisions[n*i+j+1], color='blue', linewidth=1.0)\n",
    "        cells[i, j].set_xlabel('recall', fontsize=14)\n",
    "        cells[i, j].set_ylabel('precision', fontsize=14)\n",
    "        cells[i, j].grid(True)\n",
    "        cells[i, j].set_xticks(np.linspace(0,1,11))\n",
    "        cells[i, j].set_yticks(np.linspace(0,1,11))\n",
    "        cells[i, j].set_title(\"{}, AP: {:.3f}\".format(class_names[n*i+j+1], average_precisions[n*i+j+1]), fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: Set the generator for the predictions.\n",
    "\n",
    "predict_generator = val_dataset.generate(batch_size=1,\n",
    "                                         shuffle=True,\n",
    "                                         transformations=[convert_to_3_channels,\n",
    "                                                          resize],\n",
    "                                         label_encoder=None,\n",
    "                                         returns={'processed_images',\n",
    "                                                  'inverse_transform',\n",
    "                                                  'original_images',\n",
    "                                                  'original_labels'},\n",
    "                                         keep_images_without_gt=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate batch_items\n",
    "batch_images, batch_inverse_transforms, batch_original_images, batch_original_labels = next(predict_generator)\n",
    "\n",
    "i = 0 # Which batch item to look at\n",
    "\n",
    "#print(\"Image:\", batch_filenames[i])\n",
    "print()\n",
    "print(\"Ground truth boxes:\\n\")\n",
    "print(np.array(batch_original_labels[i]))\n",
    "\n",
    "\n",
    "# 3: Make predictions.\n",
    "y_pred = model.predict(batch_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4: Decode labels\n",
    "reload(detector_help)\n",
    "# y_pred_decoded = decode_detections(y_pred,\n",
    "#                   priors,variances,\n",
    "#                   img_height = input_H,\n",
    "#                   img_width = input_W,\n",
    "#                   confidence_thresh = 0.5,\n",
    "#                   iou_threshold = 0.45)\n",
    "prior_pyramid = detector_help.prior_id_rank(feature_map=feature_map,prior_config=prior_config)\n",
    "y_pred_decoded = detector_help.post_process(y_pred, \n",
    "                                            priors,\n",
    "                                            variances,\n",
    "                                            num_classes,\n",
    "                                            input_H, \n",
    "                                            input_W,\n",
    "                                            prior_pyramid,\n",
    "                                            score_thresh = 0.1,\n",
    "                                            iou_thresh = 0.4)\n",
    "y_pred_decoded_inv = apply_inverse_transforms(y_pred_decoded, batch_inverse_transforms)\n",
    "np.set_printoptions(precision=2, suppress=True, linewidth=90)\n",
    "print(\"Predicted boxes:\\n\")\n",
    "print('   class   conf xmin   ymin   xmax   ymax  map_id')\n",
    "print(y_pred_decoded_inv[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 5: Draw \n",
    "colors = plt.cm.hsv(np.linspace(0, 1, num_classes+1)).tolist()\n",
    "draw_detection(batch_original_images[i],y_pred_decoded_inv[i],class_names,\n",
    "               show = True, draw_score = True,use_cm = True,color = colors, size = 'medium')\n",
    "draw_detection(batch_original_images[i],batch_original_labels[i],class_names, size = 'medium')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_file = \"data/dog3.jpg\"\n",
    "img = cv2.imread(img_file)\n",
    "img = cv2.resize(img,(224,224))\n",
    "x = cv2.resize(img, dsize=(224, 224), interpolation=cv2.INTER_CUBIC)\n",
    "x = np.array(x,dtype='float')\n",
    "x = x / 255\n",
    "y_pred = model.predict(np.expand_dims(x,0))\n",
    "y_pred = post_process(y_pred,priors,num_classes,input_H,input_W)\n",
    "print(y_pred)\n",
    "draw_detection(img,y_pred[0],class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video detection demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_file = \"data/cat_video.mp4\"\n",
    "result_file = \"res.avi\"\n",
    "cap= cv2.VideoCapture(video_file)\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "size = (int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n",
    "frames = (int(cap.get(cv2.CAP_PROP_FRAME_COUNT)))\n",
    "print('Input Video Size:',size,' Fps:',fps,' Frames:',frames)\n",
    "writer = cv2.VideoWriter(result_file,cv2.VideoWriter_fourcc(*'MJPG'), fps, size)\n",
    "success,frame = cap.read()\n",
    "count = 1\n",
    "while success:\n",
    "    x = cv2.resize(frame,dsize=(input_H,input_W),interpolation=cv2.INTER_CUBIC)\n",
    "    x = x / 255\n",
    "    y_pred = model.predict(np.expand_dims(x,0))[0]\n",
    "    y_pred [-4:] *= [*size,*size]\n",
    "    draw_detection(frame, y_pred,class_names,font_scale = 1)\n",
    "    writer.write(frame)\n",
    "    cv2.waitKey(1)\n",
    "    count += 1\n",
    "    if count%50==0:\n",
    "        print(y_pred)\n",
    "        plt.imshow(frame)\n",
    "        plt.show()\n",
    "        print('.',end='')\n",
    "    success,frame = cap.read()\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
